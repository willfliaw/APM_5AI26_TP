\chapter{Analysis} \label{chap:analysis}

\section{Novelty and Contributions}

The paper introduces a novel approach to time-series kernel design using NVAR. This method stands out for its integration of kernel design techniques, RC principles and the NVAR framework, a combination that enhances both interpretability and efficiency in time-series data analysis. Traditional RC-based kernels rely heavily on recurrent structures that are complex and sensitive to hyperparameter tuning, often demanding high computational resources to achieve optimal performance. In contrast, the NVAR kernel circumvents the need for recurrence by structuring embeddings as non-recursive transformations of the input data: time-delays and nonlinear functionals, such as products \cite{gauthier2021}.

This approach not only simplifies the model architecture but also reduces the dependency on interpretatively opaque hyperparameters, making it easier to apply and understand. Thus, it represents a significant advancement in kernel design by developing an NVAR-based kernel suitable for both univariate (UTS) and multivariate time-series (MTS) data, with parameter settings guided by simple heuristics. Experiments across diverse datasets show that this NVAR kernel achieves accuracy comparable to the state-of-the-art (SOTA) in time-series classification while offering a substantial improvement in computational efficiency. This balance between accuracy and speed highlights its practicality for real-world applications, especially where computational resources are limited.

From the perspective of RC, the NVAR kernel introduces a non-recursive model that circumvents the challenges of hyperparameter optimization typically faced with RC-based kernels. By forgoing recurrence, the NVAR kernel remains interpretable and simplifies the model architecture without sacrificing the quality of representation. This change not only enhances interpretability but also positions the NVAR kernel as a more accessible tool for practitioners who might otherwise face the complexities and resource demands of RC.

The paper also expands the use of NVAR beyond its traditional role in forecasting chaotic, noise-free systems, demonstrating its applicability to real-world time-series data. By connecting the method to foundational principles in dynamical systems theory, including Takens' theorem and state-space reconstruction, the paper provides a theoretical basis for understanding the embeddings used in the NVAR kernel. This connection to established theory underscores the method's robustness and supports its potential as a versatile tool in machine learning for tasks requiring sophisticated temporal representations.

\section{Methodology}

The methodology of the NVAR kernel is structured around transforming time-series data using a series of lagged embeddings and nonlinear transformations, creating a high-dimensional representation that effectively captures the underlying temporal dynamics. Specifically, the kernel leverages delay embeddings, a technique rooted in dynamical systems theory, to form feature vectors that encapsulate the past states of the time-series data. These feature vectors are then mapped to a high-dimensional space, where a similarity measure is calculated to define the kernel.

This embedding process follows Takens' theorem, which suggests that a time-delayed version of a series can reveal its latent dynamics. In this framework, each time series is enriched with delayed copies and nonlinear combinations of the input, allowing the NVAR kernel to uncover complex patterns and dependencies that traditional kernels might overlook. The paper further simplifies the hyperparameter space by employing a heuristic-based approach for setting the lag length and polynomial order, avoiding the computational burden typically associated with exhaustive tuning.

The NVAR kernel also builds on the kernel trick, which allows the computation of inner products in a transformed feature space without explicitly computing the transformation. By integrating the NVAR embedding structure into the kernel trick, the paper effectively combines the advantages of kernel-based methods with those of RC, resulting in a versatile and efficient approach to time-series analysis.

\section{Comparison to State-of-the-Art}

The proposed NVAR kernel distinguishes itself from existing time-series similarity measures and kernel methods, particularly those based on RC. Traditional RC-based kernels, such as Echo State Networks (ESNs), rely on recurrent structures that introduce complexity in hyperparameter optimization and high sensitivity to initial conditions. These recurrent models often struggle with interpretability, as their performance heavily depends on tuning multiple hyperparameters related to the reservoir's size, connectivity, and spectral properties.

In comparison, the NVAR kernel simplifies the representation by using a non-recursive structure, significantly reducing the number of hyperparameters that require fine-tuning. Unlike elastic measures, which directly measure similarity in the input space and can overlook deeper temporal dependencies, the NVAR kernel captures underlying dynamics through delay embeddings, improving its ability to handle time distortions and shifts. Additionally, while model-based kernels such as the Time Cluster Kernel (TCK) can achieve high accuracy, they tend to be computationally intensive and may not scale well with larger datasets. The NVAR kernel, in contrast, offers a compromise between accuracy and computational efficiency, making it well-suited for applications where computational resources are limited.

\section{Strengths}

The strengths of the NVAR kernel are evident in its computational efficiency, scalability, and suitability for small datasets. The non-recursive nature of the kernel design eliminates the iterative computations required in recurrent models, allowing it to scale linearly with the number of time series and reducing overall computational cost. This efficiency makes the NVAR kernel particularly valuable in contexts where fast processing is essential, such as real-time monitoring or applications with limited computational resources.

Moreover, the simplicity of the NVAR kernel's hyperparameters contributes to its scalability and ease of use. By relying on a heuristic for setting lag and polynomial order, the kernel can be quickly adapted to a variety of datasets without extensive optimization, a feature that is especially useful when working with small datasets where overfitting risks are higher. The interpretability of the NVAR approach also adds to its strengths, as the non-recursive embeddings make it easier to understand and analyze the extracted temporal features compared to traditional RC methods.

\section{Weaknesses}

Despite its advantages, the NVAR kernel has some limitations. One potential drawback is its reliance on heuristic-based hyperparameter settings, which may not always yield optimal results, particularly for highly complex datasets or data with irregular temporal patterns. While the heuristics provide a practical solution, they may oversimplify the selection of critical parameters such as lag size, leading to suboptimal embeddings in certain cases.

Another limitation lies in the method's performance with high-dimensional datasets. As the number of dimensions in the input data increases, the kernel may face challenges due to the curse of dimensionality, potentially requiring adjustments to its feature selection strategy. Additionally, the NVAR kernel's dependence on delay embeddings may restrict its applicability in situations where the time-series data lacks well-defined temporal dependencies or exhibits chaotic behavior that is difficult to model with fixed embeddings.

Overall, while the NVAR kernel offers significant improvements over traditional RC-based kernels, further work could be done to refine its hyperparameter optimization process and to extend its applicability to a broader range of complex, high-dimensional datasets.
