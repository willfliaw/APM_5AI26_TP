@inproceedings{abdullah2020,
  author    = {Abdullah, Muhammad and Ahmad, Mobeen and Han, Dongil},
  booktitle = {2020 International Conference on Electronics, Information, and Communication (ICEIC)},
  title     = {Facial Expression Recognition in Videos: An CNN-LSTM based Model for Video Classification},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1-3},
  doi       = {10.1109/ICEIC49074.2020.9051332}
}

@inproceedings{anusha2021,
  author    = {Anusha, R. and Subhashini, P. and Jyothi, Darelli and Harshitha, Potturi and Sushma, Janumpally and Mukesh, Namsamgari},
  booktitle = {2021 5th International Conference on Trends in Electronics and Informatics (ICOEI)},
  title     = {Speech Emotion Recognition using Machine Learning},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {1608-1612},
  doi       = {10.1109/ICOEI51242.2021.9453028}
}

@inproceedings{churaev2021,
  author    = {Churaev, Egor and Savchenko, Andrey V.},
  booktitle = {2021 International Russian Automation Conference (RusAutoCon)},
  title     = {Touching the Limits of a Dataset in Video-Based Facial Expression Recognition},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {633-638},
  doi       = {10.1109/RusAutoCon52004.2021.9537388}
}

@inproceedings{emodb,
  author  = {Burkhardt, Felix and Paeschke, Astrid and Rolfes, M. and Sendlmeier, Walter and Weiss, Benjamin},
  year    = {2005},
  month   = {January},
  pages   = {1517-1520},
  title   = {A database of \uppercase{G}erman emotional speech},
  volume  = {5},
  journal = {9th European Conference on Speech Communication and Technology}
}

@inproceedings{felice2023time,
  title     = {Time Series Kernels based on Nonlinear Vector AutoRegressive Delay Embeddings},
  author    = {Giovanni De Felice and John Y Goulermas and Vladimir Gusev},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year      = {2023},
  url       = {https://openreview.net/forum?id=UBUWFEwn7p}
}

@inproceedings{kotti2008,
  author    = {Kotti, Margarita and Kotropoulos, Constantine},
  booktitle = {2008 19th International Conference on Pattern Recognition},
  title     = {Gender classification in two Emotional Speech databases},
  year      = {2008},
  volume    = {},
  number    = {},
  pages     = {1-4},
  doi       = {10.1109/ICPR.2008.4761624}
}

@article{ravdess,
  doi       = {10.1371/journal.pone.0196391},
  author    = {Livingstone, Steven R. AND Russo, Frank A.},
  journal   = {PLOS ONE},
  publisher = {Public Library of Science},
  title     = {The \uppercase{R}yerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in \uppercase{N}orth \uppercase{A}merican \uppercase{E}nglish},
  year      = {2018},
  month     = {05},
  volume    = {13},
  url       = {https://doi.org/10.1371/journal.pone.0196391},
  pages     = {1-35},
  abstract  = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
  number    = {5}
}

@inproceedings{sinith2015,
  author    = {Sinith, M. S. and Aswathi, E. and Deepa, T. M. and Shameema, C. P. and Rajan, Shiny},
  booktitle = {2015 IEEE Recent Advances in Intelligent Computational Systems (RAICS)},
  title     = {Emotion recognition from audio signals using Support Vector Machine},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {139-144},
  doi       = {10.1109/RAICS.2015.7488403}
}

@inproceedings{vimal2021,
  author    = {Vimal, B. and Surya, Muthyam and Darshan and Sridhar, V.S. and Ashok, Asha},
  booktitle = {2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  title     = {MFCC Based Audio Classification Using Machine Learning},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {1-4},
  doi       = {10.1109/ICCCNT51525.2021.9579881}
}

@inproceedings{ying2010,
  author    = {Ying, Sun and Zhang, Xueying},
  booktitle = {2010 First International Conference on Pervasive Computing, Signal Processing and Applications},
  title     = {A Study of Zero-Crossings with Peak-Amplitudes in Speech Emotion Classification},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {328-331},
  doi       = {10.1109/PCSPA.2010.86}
}
